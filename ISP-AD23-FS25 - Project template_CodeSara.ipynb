{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ae0ad9-f22c-4783-b782-b5d5358132b6",
   "metadata": {},
   "source": [
    "# **Project work: Title**\n",
    "\n",
    "**Module:** Image and Signal Processing (ISP-AD23-FS25)  \n",
    "**Authors:** H√§dener Anja, Heini Sara, Huber Yeji       \n",
    "**Date:** 14.05.2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecfbd9",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "**What problem are you addressing, and why is it relevant?**     \n",
    "\n",
    "In pharmaceutical manufacturing, especially during the production of pre-filled syringes and similar medical devices, visual inspection for quality control is a mandatory and highly critical step. Currently, this process is carried out manually by trained personnel who inspect each syringe for potential defects such as air bubbles, foreign particles, or missing liquid content.     \n",
    "However, manual inspection poses several challenges: it requires significant training time, is susceptible to human error, and leads to rapid visual fatigue. In real-world operations, inspectors typically need to take a break after 30 minutes of work due to eye strain. Additionally, to maximize production efficiency, manufacturing lines often run continuously‚Äîincluding overnight shifts‚Äîwhich makes it difficult and costly to rely exclusively on human labor.     \n",
    "If this process could be automated, it could address many of these issues: reducing training overhead, minimizing errors, supporting 24/7 operations, and improving overall manufacturing throughput and consistency.\n",
    "\n",
    "**What kind of image or signal data are you working with?**     \n",
    "\n",
    "We are working with RGB image data captured via webcam. The dataset includes images of syringes in various conditions:\n",
    "\n",
    "(1) Reference syringes (usable)     \n",
    "(2) Syringes containing air bubbles      \n",
    "(3) Syringes with foreign objects     \n",
    "(4) Empty syringes     \n",
    "\n",
    "These images are used for both training and evaluating a classification model that determines whether a syringe is usable or defective.\n",
    "\n",
    "**What is the main goal of your processing task?**     \n",
    "\n",
    "The main objective of our image processing task is to build an automated system that analyzes real-time images of syringes and determines whether each one meets usability criteria. By comparing the live input to reference examples using computer vision techniques, the system should be able to detect visual anomalies such as air bubbles, contamination, or missing liquid. Ultimately, the goal is to support or replace human inspection in pharmaceutical quality control‚Äîensuring consistent accuracy while enabling \n",
    "high-throughput and around-the-clock production.\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "<b>Instructions:</b>\n",
    "Use this section to introduce and motivate your project. Provide enough context for fellow students to understand your goals.\n",
    "\n",
    "<ul style=\"color:#2D8FF3\">\n",
    "    <li>What problem are you addressing, and why is it relevant?</li>\n",
    "    <li>What kind of image or signal data are you working with?</li>\n",
    "    <li>What is the main goal of your processing task?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea206c6",
   "metadata": {},
   "source": [
    "---\n",
    "## **Setup**\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "<b>Instructions:</b>\n",
    "This section is about configuring the Jupyter Notebook.  \n",
    "You don't need to do much here ‚Äì just make sure everything runs correctly.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afaf04a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"265.5pt\" height=\"42.12pt\" viewBox=\"0 0 265.5 42.12\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-05-30T10:31:50.050087</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 42.12 \n",
       "L 265.5 42.12 \n",
       "L 265.5 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 7.2 34.92 \n",
       "L 258.3 34.92 \n",
       "L 258.3 7.2 \n",
       "L 7.2 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pdd95c69926)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAABBcAAAB0CAYAAADek9rWAAADQklEQVR4nO3YsU0CUABFUaSXip7SSUxM6EkomMaGGexhC7ewtiWxJ8ZY6BD3Jz+QcyZ47bsPu9P1b8HdOH8cZ09gsO3zz+wJDLRfvc6ewECHy/vsCQz29vQ7ewIDvay3sycw0Ob7a/YEBvt8dEXvyXL2AAAAAOC2iQsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIm4AAAAACTiAgAAAJCICwAAAEAiLgAAAACJuAAAAAAk4gIAAACQiAsAAABAIi4AAAAAibgAAAAAJOICAAAAkIgLAAAAQCIuAAAAAIviH1UgEJo1prRpAAAAAElFTkSuQmCC\" id=\"image701f948b80\" transform=\"scale(1 -1) translate(0 -27.84)\" x=\"7.2\" y=\"-7.08\" width=\"251.28\" height=\"27.84\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 7.2 34.92 \n",
       "L 7.2 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 35.1 34.92 \n",
       "L 35.1 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 63 34.92 \n",
       "L 63 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 90.9 34.92 \n",
       "L 90.9 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 118.8 34.92 \n",
       "L 118.8 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 146.7 34.92 \n",
       "L 146.7 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 174.6 34.92 \n",
       "L 174.6 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 202.5 34.92 \n",
       "L 202.5 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 230.4 34.92 \n",
       "L 230.4 7.2 \n",
       "\" clip-path=\"url(#pdd95c69926)\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\"/>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 7.2 34.92 \n",
       "L 7.2 7.2 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 258.3 34.92 \n",
       "L 258.3 7.2 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 7.2 34.92 \n",
       "L 258.3 34.92 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 7.2 7.2 \n",
       "L 258.3 7.2 \n",
       "\" style=\"fill: none; stroke: #cccccc; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdd95c69926\">\n",
       "   <rect x=\"7.2\" y=\"7.2\" width=\"251.1\" height=\"27.72\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1350x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Enable vectorized graphics\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup plotting\n",
    "PALETTE = [ (0.341, 0.648, 0.962, 1.0), \n",
    "            (0.990, 0.476, 0.494, 1.0), \n",
    "            (0.281, 0.749, 0.463, 1.0), \n",
    "            (0.629, 0.802, 0.978, 1.0), \n",
    "            (0.994, 0.705, 0.715, 1.0), \n",
    "            (0.595, 0.858, 0.698, 1.0), \n",
    "            (0.876, 0.934, 0.992, 1.0), \n",
    "            (0.998, 0.901, 0.905, 1.0), \n",
    "            (0.865, 0.952, 0.899, 1.0) ]\n",
    "\n",
    "sns.palplot(PALETTE, size=0.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=PALETTE)\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d519f",
   "metadata": {},
   "source": [
    "---\n",
    "## **Implementation**\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "<b>Instructions:</b>\n",
    "In this section, explain how you implemented your solution, focusing on the key processing steps and how they were translated into code. Justify your design choices and parameter settings, and describe any challenges you encountered during development. Structure this part like a tutorial to make it easy for fellow students to follow and learn from your approach.\n",
    "\n",
    "<ul style=\"color:#2D8FF3\">\n",
    "    <li>Which key functions, algorithms, or libraries (e.g., NumPy, OpenCV, librosa, etc.) did you use?</li>\n",
    "    <li>How is your processing pipeline structured? (e.g., filtering ‚Üí transformation ‚Üí output)</li>\n",
    "    <li>Explain any relevant design decisions (e.g., kernel size, interpolation method, threshold values).</li>\n",
    "    <li>If you implemented parts yourself (e.g., a filter or transform), briefly explain how.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Hint:</b> To keep your code clean and modular, encapsulate your processing or feature (as well as the intermediate processing steps) into functions. See the example below for illustration\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b46f5",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This project was implemented in Python using several standard libraries for image processing and machine learning, primarily **OpenCV**, **NumPy**, **Matplotlib**, and **scikit-learn**.\n",
    "\n",
    "### 1. Key Libraries and Tools\n",
    "- **OpenCV**: Used for live video capture, image preprocessing, contour detection, and drawing.\n",
    "- **NumPy**: Used for numerical operations and array manipulations.\n",
    "- **Matplotlib** and **Seaborn**: Used for visualization and plotting.\n",
    "- **scikit-learn**: Used to implement a basic machine learning pipeline (Random Forest classifier).\n",
    "\n",
    "### 2. Processing Pipeline Structure\n",
    "The processing pipeline is modular and follows this structure:\n",
    "\n",
    "1. **Capture**: A live video feed is accessed via `cv.VideoCapture(0)`.\n",
    "2. **Preprocessing**: Each frame is converted to grayscale, smoothed using a Gaussian blur, and processed using Canny edge detection.\n",
    "3. **Contour Detection**: External contours are extracted from the edge map using `cv.findContours()`.\n",
    "4. **Filtering**: Only contours within a specified area range are retained to eliminate noise and background elements.\n",
    "5. **Bounding and ROI Saving**: Valid contours are highlighted on the video stream, and the corresponding regions (ROIs) are saved to disk.\n",
    "6. **Machine Learning Integration** (offline): A `RandomForestClassifier` is trained to distinguish between ‚Äúok‚Äù and ‚Äúdefect‚Äù syringe samples.\n",
    "\n",
    "Each step is encapsulated in a Python function for clarity and reusability.\n",
    "\n",
    "### 3. Design Decisions and Parameters\n",
    "- **Canny edge thresholds**: `(30, 100)` for sensitivity to small defects.\n",
    "- **Gaussian blur kernel size**: `(5, 5)` to reduce noise while maintaining edge quality.\n",
    "- **Contour area filtering**: Only areas between `500` and `30000` pixels are considered relevant.\n",
    "- **ROI resize for ML**: Each image is resized to `64√ó64` for training, balancing detail with performance.\n",
    "\n",
    "### 4. Challenges Encountered\n",
    "- The syringe needs to be held very close to the camera to detect fine surface defects.\n",
    "- Initial Canny thresholds were too high and failed to capture small scratches or cracks.\n",
    "- Uneven lighting conditions introduced noise; consistent illumination improved accuracy.\n",
    "- Automating the ROI extraction and naming was necessary for efficient dataset generation.\n",
    "\n",
    "### 5. Modularity\n",
    "All main processing steps are implemented as standalone functions:\n",
    "\n",
    "- `preprocess_frame(frame)`\n",
    "- `find_valid_contours(edges)`\n",
    "- `draw_bounding_boxes(frame, contours)`\n",
    "- `save_rois(frame, contours, frame_id)`\n",
    "- `load_dataset(base_path)`\n",
    "\n",
    "This structure allows for easy extension, reuse, and integration of additional logic (e.g., classification, augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e61028",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# üìç Modellpfade anpassen\u001b[39;00m\n\u001b[32m     11\u001b[39m rf_model_path = \u001b[33m'\u001b[39m\u001b[33msyringe_model_dataset_weighted.pkl\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Pfad zu deinem Random Forest\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# üìç Modellpfade anpassen\n",
    "rf_model_path = 'syringe_model_dataset_weighted.pkl'  # Pfad zu deinem Random Forest\n",
    "cnn_model_path = 'cnn_model.h5'  # Falls du das CNN gespeichert hast\n",
    "\n",
    "# üì¶ Modelle laden\n",
    "rf_model = joblib.load(rf_model_path)\n",
    "cnn_model = load_model(cnn_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8c49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is running. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "def run_detection():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    frame_count = 0\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Camera not accessible.\")\n",
    "        return\n",
    "\n",
    "    print(\"Camera is running. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame capture failed.\")\n",
    "            break\n",
    "\n",
    "        edges = preprocess_frame(frame)\n",
    "        valid_contours = find_largest_contour(edges)\n",
    "        if valid_contours:\n",
    "            display_frame = draw_bounding_boxes(frame.copy(), valid_contours)\n",
    "            save_rois(frame, valid_contours, frame_count)\n",
    "        else:\n",
    "            display_frame = frame.copy()\n",
    "\n",
    "\n",
    "        cv.imshow(\"Detected Regions\", display_frame)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "# Start detection\n",
    "run_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43519bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_path=\"dataset\", size=(64, 64)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label_name in [\"ok\", \"defect\"]:\n",
    "        folder = os.path.join(base_path, label_name)\n",
    "        for file in os.listdir(folder):\n",
    "            path = os.path.join(folder, file)\n",
    "            img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv.resize(img, size)\n",
    "            data.append(img.flatten())\n",
    "            labels.append(label_name)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c1fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_liquid_level(edges):\n",
    "    # Detect horizontal lines that could indicate liquid level\n",
    "    lines = cv.HoughLinesP(edges, 1, np.pi/180, 50, minLineLength=100, maxLineGap=10)\n",
    "    return lines is not None\n",
    "\n",
    "def detect_bubbles(gray):\n",
    "    # Use circle detection to find bubbles\n",
    "    circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20,\n",
    "                            param1=50, param2=30, minRadius=5, maxRadius=30)\n",
    "    return circles is not None\n",
    "\n",
    "def detect_particles(gray):\n",
    "    # Use contour detection with area filtering for particles\n",
    "    _, thresh = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)\n",
    "    contours, _ = cv.findContours(thresh, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
    "    return any(cv.contourArea(c) < 100 for c in contours)  # Small contours might be particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc789372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(image):\n",
    "    \"\"\"\n",
    "    Augment a single image with various transformations to multiply the dataset.\n",
    "    Returns a list of augmented images.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    # augment rotations at finer angles\n",
    "    for angle in range(0, 360, 30):  # Rotate every 30 degrees\n",
    "        matrix = cv.getRotationMatrix2D((image.shape[1]/2, image.shape[0]/2), angle, 1.0)\n",
    "        rotated = cv.warpAffine(image, matrix, (image.shape[1], image.shape[0]))\n",
    "        augmented.append(rotated)\n",
    "    \n",
    "    # augment flips and combinations\n",
    "    flipped_h = cv.flip(image, 1)\n",
    "    flipped_v = cv.flip(image, 0)\n",
    "    flipped_both = cv.flip(image, -1)\n",
    "    augmented.extend([flipped_h, flipped_v, flipped_both])\n",
    "    \n",
    "    # augment noise levels\n",
    "    for sigma in [10, 20, 30]:\n",
    "        noise = np.random.normal(0, sigma, image.shape).astype(np.uint8)\n",
    "        noisy = cv.add(image, noise)\n",
    "        augmented.append(noisy)\n",
    "    \n",
    "    # augment brightness adjustments\n",
    "    for delta in [-50, -25, 25, 50]:\n",
    "        if delta > 0:\n",
    "            adjusted = cv.add(image, delta)\n",
    "        else:\n",
    "            adjusted = cv.subtract(image, abs(delta))\n",
    "        augmented.append(adjusted)\n",
    "    \n",
    "    # augment blur levels\n",
    "    for kernel_size in [(3,3), (5,5), (7,7)]:\n",
    "        blurred = cv.GaussianBlur(image, kernel_size, 0)\n",
    "        augmented.append(blurred)\n",
    "    \n",
    "    # Contrast adjustments\n",
    "    for alpha in [0.5, 1.5]:  # Contrast factors\n",
    "        contrasted = cv.convertScaleAbs(image, alpha=alpha, beta=0)\n",
    "        augmented.append(contrasted)\n",
    "    \n",
    "    # Combinations of transformations\n",
    "    for rot_img in augmented[:3]:  # Take first few rotated images\n",
    "        noise = np.random.normal(0, 15, rot_img.shape).astype(np.uint8)\n",
    "        combo = cv.add(rot_img, noise)\n",
    "        combo = cv.GaussianBlur(combo, (3,3), 0)\n",
    "        augmented.append(combo)\n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3aed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 58\n",
      "Augmented dataset size: 1798\n",
      "\n",
      "Model performance with augmented training data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      defect       0.95      1.00      0.98       303\n",
      "          ok       1.00      0.74      0.85        57\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.98      0.87      0.91       360\n",
      "weighted avg       0.96      0.96      0.96       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset and augment\n",
    "X_orig, y_orig = load_dataset()\n",
    "\n",
    "# Augment data\n",
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "for img, label in zip(X_orig, y_orig):\n",
    "    # Reshape flattened image back to 2D\n",
    "    img_2d = img.reshape(64, 64)\n",
    "    \n",
    "    # Add original image\n",
    "    X_augmented.append(img)\n",
    "    y_augmented.append(label)\n",
    "    \n",
    "    # Add augmented versions\n",
    "    augmented_images = augment_data(img_2d)\n",
    "    for aug_img in augmented_images:\n",
    "        X_augmented.append(aug_img.flatten())\n",
    "        y_augmented.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X_augmented)\n",
    "y = np.array(y_augmented)\n",
    "\n",
    "print(f\"Original dataset size: {len(X_orig)}\")\n",
    "print(f\"Augmented dataset size: {len(X)}\")\n",
    "\n",
    "# Split augmented data into train/test sets\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model on augmented data\n",
    "clf_aug = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_aug.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "# Evaluate model trained on augmented data\n",
    "y_pred_aug = clf_aug.predict(X_test_aug)\n",
    "print(\"\\nModel performance with augmented training data:\")\n",
    "print(classification_report(y_test_aug, y_pred_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db855f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_roi(image, clf, size=(64, 64)):\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    resized = cv.resize(gray, size).flatten().reshape(1, -1)\n",
    "    return clf.predict(resized)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e64d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Results**\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "<b>Instructions:</b>\n",
    "Demonstrate the outcome of your solution. This section can be brief, but should show what your implementation produces.\n",
    "\n",
    "<ul style=\"color:#2D8FF3\">\n",
    "    <li>Use representative input and output examples to illustrate the effect of your processing.</li>\n",
    "    <li>Don't limit yourself to best-case results ‚Äì also show cases where the method performs poorly or struggles.</li>\n",
    "</ul>\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a69cdae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_and_predict_rois' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load and predict on detected_rois with augmented model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m predictions = \u001b[43mload_and_predict_rois\u001b[49m(clf_aug)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions:\n",
      "\u001b[31mNameError\u001b[39m: name 'load_and_predict_rois' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and predict on detected_rois with augmented model\n",
    "predictions = load_and_predict_rois(clf_aug)\n",
    "\n",
    "# Print summary\n",
    "if predictions:\n",
    "    ok_count = sum(1 for pred in predictions.values() if pred == \"ok\")\n",
    "    defect_count = sum(1 for pred in predictions.values() if pred == \"defect\")\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total ROIs processed: {len(predictions)}\")\n",
    "    print(f\"OK syringes: {ok_count}\")\n",
    "    print(f\"Defective syringes: {defect_count}\")\n",
    "else:\n",
    "    print(\"\\nNo ROIs were processed. Make sure there are PNG images in the 'detected_rois' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1638d63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Discussion**\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "<b>Instructions:</b>\n",
    "Reflect on your results and the overall performance of your solution. This section helps you and others understand what worked well, what did not, and why.\n",
    "\n",
    "<ul>\n",
    "    <li>Interpret your results: What patterns or weaknesses did you observe?</li>\n",
    "    <li>How robust is your solution to variations in the input (e.g., noise, contrast, lighting)?</li>\n",
    "    <li>Were there cases where your method failed or produced unexpected results? Why?</li>\n",
    "    <li>What could be improved or extended in a future version?</li>\n",
    "    <li>If applicable, compare your method to alternatives or standard approaches.</li>\n",
    "</ul>\n",
    "\n",
    "Be honest and analytical ‚Äì this section is not about perfection, but about insight.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ea149",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Appendix**\n",
    "\n",
    "### **How to convert a Jupyter Notebook into a PDF**\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:#2D8FF3;font-style:italic\">\n",
    "\n",
    "<ul>\n",
    "    <li>Run the entire Jupyter notebook and save it.</li>\n",
    "    <li>Open a terminal and run the following command.<br>\n",
    "       <tt></tt>\n",
    "    </li>\n",
    "     <div style=\"border: 1px solid #ccc; \n",
    "                 color: #A0A0A0;\n",
    "                 padding: 4px; \n",
    "                 background-color: #f9f9f9; \n",
    "                 border-radius: 4px; \n",
    "                 width: 100%;\n",
    "                 max-width: 60%;\">\n",
    "        <tt>jupyter nbconvert --to html \"path/to/your/notebook.ipynb\"</tt>.\n",
    "    </div>\n",
    "    <li>This creates a file notebook.html in the current working directory.</li>\n",
    "    <li>Open the document in a web browser (the Opera browser works best, as it saves single-page PDFs!)</li>\n",
    "    <li>Save as PDF</li>\n",
    "</ul>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
